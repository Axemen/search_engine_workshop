{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0fcbf6cc5b55707655e96a5b5e596f8c95e06f374d66cc86f92aa22144f17e6e1",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Describing the inverted index\n",
    "\n",
    "The first thing that we're going to be doing is setting up an inverted index. This index will be used to map the term frequency to a particular document in a nested dictionary structure. \n",
    "\n",
    "```\n",
    "index = {\n",
    "    term : {\n",
    "        document_id: term frequency\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "We're also going to create a dictionary that will hold the metadata of our documents\n",
    "\n",
    "```\n",
    "documents = {\n",
    "    document_id: {\n",
    "        name: name of the document,\n",
    "        magnitude: length of the vector # This will be important later. \n",
    "    }\n",
    "}\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}\n",
    "documents = {}"
   ]
  },
  {
   "source": [
    "# Ingestion of documents\n",
    "\n",
    "Now we're going to establish the procedure for document ingestion. This will take in a document and will add it to our index. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First we're going to read in a short article from wikipedia"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"[ Emma by Jane Austen 1816 ] VOLUME I CHAPTER I Emma Woodhouse , handsome , clever , and rich , with a comfortable home and happy disposition , seemed to unite some of the best blessings of existence ; and had lived nearly twenty - one years in the world with very little to distress or vex her . She was the youngest of the two daughters of a most affectionate , indulgent father ; and had , in consequence of her sister ' s marriage , been mistress of his house from a very early period . Her mother had died too long ago for her to have more than an indistinct remembrance of her caresses ; and her place had been supplied by an excellent woman as governess , who had fallen little short of a mother in affection . Sixteen years had Miss Taylor been in Mr . Woodhouse ' s family , less as a governess than a friend , very fond of both daughters , but particularly of Emma . Between _them_ it was more the intimacy of sisters . Even before Miss Taylor had ceased to hold the nominal office of gover\""
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "text = open(\"articles/austen-emma.txt\", 'r', encoding='utf-8').read()\n",
    "text[:1000]"
   ]
  },
  {
   "source": [
    "For this text we're going to have to go through a few different steps in order to ingest it properly.\n",
    "\n",
    "1. Tokenize the text\n",
    "2. Calculate the frequency of the tokens\n",
    "3. add the terms and their frequencies to the inverted index\n",
    "4. calculate the magnitude of the document's term vector\n",
    "5. add the document's metadata to the documents dictionary\n",
    "\n",
    "We're calculating the magnitude of the term vector now since it is easier to calculate it now rather than at the time of ranking"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "# Using a simple regular expression in order to tokenize.\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['emma', 'by', 'jane', 'austen', '1816']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "tokens = tokenize(text)\n",
    "tokens[:5]"
   ]
  },
  {
   "source": [
    "In order to count the frequencies we're going to use the collections class from counter which does exactly that and returns a dictionary like object that we can use to reference the term frequencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from collections import Counter\n",
    "\n",
    "term_frequencies = Counter(tokens)\n",
    "term_frequencies.most_common(5)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('to', 5239), ('the', 5201), ('and', 4896), ('of', 4291), ('i', 3178)]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "source": [
    "Now for each unique term from the text we need to add it to the index by reference of a document id like so"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = len(documents) # By taking the len(documents) we get an incrementing identifier\n",
    "\n",
    "for term, term_freq in term_frequencies.items():\n",
    "    if term not in index:\n",
    "        index[term] = {}\n",
    "    index[term][doc_id] = term_freq"
   ]
  },
  {
   "source": [
    "Now let's go ahead and calculate the magnitude of the term vector"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14364.35230005168"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from math import sqrt\n",
    "mag = sqrt(sum([x**2 for x in term_frequencies.values()]))\n",
    "mag"
   ]
  },
  {
   "source": [
    "Now that we have the magnitude let's go ahead and add it as part of the metadata in our documents dictionary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[doc_id] = {\n",
    "    \"name\": \"Mariembourg.txt\",\n",
    "    \"magnitude\": mag\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: {'name': 'Mariembourg.txt', 'magnitude': 14364.35230005168}}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "source": [
    "Now that we have the procedure established let's go ahead and turn them into functions that can take in some text and automatically add them to the index "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_document(text:str, name:str):\n",
    "    tokens = tokenize(text)\n",
    "    term_frequencies = Counter(tokens)\n",
    "    doc_id = len(documents)\n",
    "\n",
    "    for term in term_frequencies:\n",
    "        if term not in index:\n",
    "            index[term] = {}\n",
    "        index[term][doc_id] = term_frequencies[term]\n",
    "    \n",
    "    mag = sqrt(sum([x**2 for x in term_frequencies.values()]))\n",
    "\n",
    "    documents[doc_id] = {\n",
    "        \"name\": name,\n",
    "        \"magnitude\": mag\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting files from the articles folder. This will be created from the data_pull.py file\n",
    "\n",
    "from glob import glob\n",
    "paths = glob(\"articles/*.txt\")\n",
    "\n",
    "# resetting the index and documents dicts as to not re-read data\n",
    "index = {}\n",
    "documents = {}\n",
    "\n",
    "for path in paths:\n",
    "    text = open(path, 'r').read()\n",
    "    index_document(text, name=path)"
   ]
  },
  {
   "source": [
    "# Searching the index\n",
    "\n",
    "Now that we have a variety of documents ingested into the index we can start to query our index. To do this we're going to be calculating the cosine similarity of the query to the document within the query. This will give us a rank for each document's similairty to the query and in turn our search results. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To do this we're going to \n",
    "\n",
    "* iterate over the terms of the query\n",
    "* find the documents associated with the current term\n",
    "* add the tfidf scores for each of the individual documents\n",
    "* normalize the sum of the tfidf scores using the pre-calculated magnitude of the documents term vector\n",
    "* get the top 10 docuemnt ids from the calculated scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "articles\\chesterton-brown.txt : 0.06230762498080422\narticles\\bible-kjv.txt : 0.0031629507437594714\narticles\\blake-poems.txt : 0.0016854240823057666\narticles\\shakespeare-hamlet.txt : 0.0007197148531950618\narticles\\chesterton-ball.txt : 0.0004371842305225936\narticles\\melville-moby_dick.txt : 0.0002783200568285072\narticles\\whitman-leaves.txt : 0.000253875114132975\narticles\\chesterton-thursday.txt : 0.00020280507834880357\narticles\\milton-paradise.txt : 0.00017685509193384998\narticles\\bryant-stories.txt : 0.00011469704186578875\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "query = \"Flambeau priest\"\n",
    "query_terms = tokenize(query)\n",
    "\n",
    "N = len(documents) # The number of documents in the corpus\n",
    "scores = Counter() # Counter to hold the scores and default to zero if it doesn't exist\n",
    "\n",
    "for term in query_terms:\n",
    "    df = len(index[term]) # The document freqeuncy for the term \n",
    "    idf = log(N/df) # The inverse-document frequency for the term\n",
    "\n",
    "    for doc_id, tf in index[term].items():\n",
    "        scores[doc_id] += (tf * idf) # adding the tfidf to the document's score for this query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the scores based on the magnitude we calculated during document ingestion\n",
    "for doc_id, score in scores.items():\n",
    "    scores[doc_id] = score / documents[doc_id]['magnitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top results from the query\n",
    "for doc_id, score in scores.most_common(10):\n",
    "    print(documents[doc_id]['name'], \":\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query:str):\n",
    "    query_terms = tokenize(query)\n",
    "\n",
    "    N = len(documents) # The number of documents in the corpus\n",
    "    scores = Counter() # Counter to hold the scores and default to zero if it doesn't exist\n",
    "\n",
    "    for term in query_terms:\n",
    "        df = len(index[term]) # The document freqeuncy for the term \n",
    "        idf = log(N/df) # The inverse-document frequency for the term\n",
    "\n",
    "        for doc_id, tf in index[term].items():\n",
    "            scores[doc_id] += (tf * idf) # adding the tfidf to the document's score for this query\n",
    "    # Normalize the scores based on the magnitude we calculated during document ingestion\n",
    "    for doc_id, score in scores.items():\n",
    "        scores[doc_id] = score / documents[doc_id]['magnitude']\n",
    "\n",
    "    # get the top results from the query\n",
    "    for doc_id, score in scores.most_common(10):\n",
    "        print(documents[doc_id]['name'], \":\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "articles\\chesterton-brown.txt : 0.010797435480809386\narticles\\bible-kjv.txt : 0.0031629507437594714\narticles\\blake-poems.txt : 0.0016854240823057666\narticles\\shakespeare-hamlet.txt : 0.0007197148531950618\narticles\\chesterton-ball.txt : 0.0004371842305225936\narticles\\melville-moby_dick.txt : 0.0002783200568285072\narticles\\whitman-leaves.txt : 0.000253875114132975\narticles\\chesterton-thursday.txt : 0.00020280507834880357\narticles\\milton-paradise.txt : 0.00017685509193384998\narticles\\bryant-stories.txt : 0.00011469704186578875\n"
     ]
    }
   ],
   "source": [
    "search(\"priest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}