{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describing the inverted index\n",
    "\n",
    "The first thing that we're going to be doing is setting up an inverted index. This index will be used to map the term frequency to a particular document in a nested dictionary structure. \n",
    "\n",
    "```\n",
    "index = {\n",
    "    term : {\n",
    "        document_id: term frequency\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "We're also going to create a dictionary that will hold the metadata of our documents\n",
    "\n",
    "```\n",
    "documents = {\n",
    "    document_id: {\n",
    "        name: name of the document,\n",
    "        magnitude: length of the vector # This will be important later. \n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}\n",
    "documents = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion of documents\n",
    "\n",
    "Now we're going to establish the procedure for document ingestion. This will take in a document and will add it to our index. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we're going to read in a short article from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "evere application to study had\n\nmade upon his health; and I was happy to conclude, from the excellent\n\nphysical condition in which I saw him, that the remedy had already been\n\nattended with very desirable success. He had now run up from Boston by\n\nthe noon train, partly impelled by the friendly regard with which he\n\nis pleased to honor me, and partly, as I soon found, on a matter of\n\nliterary business.\n\n\n\nIt delighted me to receive Mr. Bright, for the first time, under a roof,\n\nthough a very hum\n"
     ]
    }
   ],
   "source": [
    "path = \"books/Tanglewood Tales by Nathaniel Hawthorne.txt\"\n",
    "text = open(path, 'r', encoding='utf-8').read()\n",
    "print(text[1000:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this text we're going to have to go through a few different steps in order to ingest it properly.\n",
    "\n",
    "1. Preprocess the text\n",
    "2. Calculate the frequency of the tokens\n",
    "3. Insert the terms and their frequencies for each document into the inverted index\n",
    "4. Calculate the magnitude of the document's term vector\n",
    "5. Insert the document's metadata into the documents dictionary\n",
    "\n",
    "We're calculating the magnitude of the term vector now since it is easier to calculate it now rather than at the time of ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "# Using a simple regular expression in order to tokenize.\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['i', 'was', 'favored', 'with', 'a', 'flying', 'visit', 'from', 'my', 'young']\nNumber of tokens: 69379\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize(text)\n",
    "print(tokens[100:110])\n",
    "print(\"Number of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to count the frequencies we're going to use the collections class from counter which does exactly that and returns a dictionary like object that we can use to reference the term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the 3948\nproject 87\ngutenberg 93\nebook 11\nof 2150\ntanglewood 9\ntales 9\nby 295\nnathaniel 4\nhawthorne 4\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "term_frequencies = Counter(tokens)\n",
    "\n",
    "for k in list(term_frequencies.keys())[:10]:\n",
    "    print(k, term_frequencies[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each unique term from the text we need to add it to the index by reference of a document id like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = len(documents) # By taking the len(documents) we get an incrementing identifier\n",
    "\n",
    "for term, term_freq in term_frequencies.items():\n",
    "    if term not in index:\n",
    "        index[term] = {}\n",
    "    index[term][doc_id] = term_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go ahead and calculate the magnitude of the term vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6665.505757255034"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from math import sqrt\n",
    "mag = sqrt(sum(x**2 for x in term_frequencies.values()))\n",
    "mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the magnitude let's go ahead and add it as part of the metadata in our documents dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[doc_id] = {\n",
    "    \"name\": path,\n",
    "    \"magnitude\": mag\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the procedure established let's go ahead and turn them into functions that can take in some text and automatically add them to the index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_document(text:str, name:str):\n",
    "    tokens = tokenize(text)\n",
    "    term_frequencies = Counter(tokens)\n",
    "    doc_id = len(documents)\n",
    "\n",
    "    for term in term_frequencies:\n",
    "        if term not in index:\n",
    "            index[term] = {}\n",
    "        index[term][doc_id] = term_frequencies[term]\n",
    "    \n",
    "    mag = sqrt(sum([x**2 for x in term_frequencies.values()]))\n",
    "\n",
    "    documents[doc_id] = {\n",
    "        \"name\": name,\n",
    "        \"magnitude\": mag\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 808/808 [00:50<00:00, 15.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Getting files from the articles folder. This will be created from the data_pull.py file\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "paths = glob(\"books/*.txt\")\n",
    "\n",
    "# resetting the index and documents dicts as to not re-read data\n",
    "index = {}\n",
    "documents = {}\n",
    "\n",
    "for path in tqdm(paths):\n",
    "    text = open(path, 'r', encoding='utf-8').read()\n",
    "    index_document(text, name=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching the index\n",
    "\n",
    "Now that we have a variety of documents ingested into the index we can start to query our index. To do this we're going to be calculating the cosine similarity of the query to the document within the query. This will give us a rank for each document's similairty to the query and in turn our search results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this we're going to \n",
    "\n",
    "* Iterate over the terms of the query\n",
    "* Find the documents associated with the current term\n",
    "* Calculate the sum of the tfidf scores for each of the individual documents\n",
    "* Normalize the sum of the tfidf scores using the pre-calculated magnitude of the documents term vector\n",
    "* Get the top 10 document ids from the calculated scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "query = \"Flambeau priest\"\n",
    "query_terms = tokenize(query)\n",
    "\n",
    "N = len(documents) # The number of documents in the corpus\n",
    "scores = Counter() # Counter to hold the scores and default to zero if it doesn't exist\n",
    "\n",
    "for term in query_terms:\n",
    "    df = len(index[term]) # The document freqeuncy for the term \n",
    "    idf = log(N/df) # The inverse-document frequency for the term\n",
    "\n",
    "    for doc_id, tf in index[term].items():\n",
    "        scores[doc_id] += (tf * idf) # adding the tfidf to the document's score for this query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the scores based on the magnitude we calculated during document ingestion\n",
    "for doc_id, score in scores.items():\n",
    "    scores[doc_id] = score / documents[doc_id]['magnitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "books\\The Innocence of Father Brown by G. K. Chesterton.txt : 0.11844505593618641\nbooks\\The Wisdom of Father Brown by G. K. Chesterton.txt : 0.07829545072133995\nbooks\\The King's Jackal by Richard Harding Davis.txt : 0.010696040353392553\nbooks\\Fables by Robert Louis Stevenson.txt : 0.009301339283403058\nbooks\\A Theological-Political Treatise [Part IV] by Benedictus de Spinoza.txt : 0.007420887002484888\nbooks\\The Damnation of Theron Ware by Harold Frederic.txt : 0.006779504784506538\nbooks\\The Confutatio Pontificia by Johann Michael Reu.txt : 0.006464333956174691\nbooks\\Ballads by Robert Louis Stevenson.txt : 0.006181985437514823\nbooks\\The Story of the Amulet by E. Nesbit.txt : 0.00588393808574806\nbooks\\The Mayflower Compact.txt : 0.005525188901532451\n"
     ]
    }
   ],
   "source": [
    "# get the top results from the query\n",
    "for doc_id, score in scores.most_common(10):\n",
    "    print(documents[doc_id]['name'], \":\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query:str):\n",
    "    query_terms = tokenize(query)\n",
    "\n",
    "    N = len(documents) # The number of documents in the corpus\n",
    "    scores = Counter() # Counter to hold the scores and default to zero if it doesn't exist\n",
    "\n",
    "    for term in query_terms:\n",
    "        df = len(index[term]) # The document freqeuncy for the term \n",
    "        idf = log(N/df) # The inverse-document frequency for the term\n",
    "\n",
    "        for doc_id, tf in index[term].items():\n",
    "            scores[doc_id] += (tf * idf) # adding the tfidf to the document's score for this query\n",
    "    # Normalize the scores based on the magnitude we calculated during document ingestion\n",
    "    for doc_id, score in scores.items():\n",
    "        scores[doc_id] = score / documents[doc_id]['magnitude']\n",
    "\n",
    "    # get the top results from the query\n",
    "    for doc_id, score in scores.most_common(10):\n",
    "        print(documents[doc_id]['name'], \":\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "books\\Orlando Furioso by Lodovico Ariosto.txt : 0.011454267647381083\nbooks\\The Song of Roland by C. K. Scott-Moncrieff.txt : 0.005918761796358551\nbooks\\Travels with a Donkey in the Cevennes by Robert Louis Stevenson.txt : 0.004376585534473633\nbooks\\Life of Robert Browning by William Sharp.txt : 0.0030481482585879337\nbooks\\The Breitmann Ballads by Charles Godfrey Leland.txt : 0.002346248211612463\nbooks\\Four Arthurian Romances by active 12th century de Troyes Chrétien.txt : 0.0014779907490621055\nbooks\\Don Quixote by Miguel de Cervantes Saavedra.txt : 0.001346239924237458\nbooks\\Reprinted Pieces by Charles Dickens.txt : 0.0012344111357089579\nbooks\\Child Christopher and Goldilind the Fair by William Morris.txt : 0.00106557836375704\nbooks\\The Goodness of St. Rocque, and Other Stories by Alice Moore Dunbar-Nelson.txt : 0.0010290289032903579\n"
     ]
    }
   ],
   "source": [
    "search(\"Roland\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python37464bitbaseconda77b3c7de2ae14481a66df9be4dc9e437",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "fcbf6cc5b55707655e96a5b5e596f8c95e06f374d66cc86f92aa22144f17e6e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}